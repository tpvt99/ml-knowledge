import torch as t
from torch import Tensor
import einops
from typing import Callable, List
from jaxtyping import Float
from transformer_lens import HookedTransformer

# FLAT SOLUTION
# YOUR CODE HERE - define `blank_probe` and `my_probe`

# FLAT SOLUTION END

import part6_othellogpt.tests as tests

def test_my_probes(blank_probe: t.Tensor, my_probe: t.Tensor, linear_probe: t.Tensor):

    blank_probe_expected = linear_probe[..., 0] - linear_probe[..., 1] * 0.5 - linear_probe[..., 2] * 0.5
    my_probe_expected = linear_probe[..., 2] - linear_probe[..., 1]

    t.testing.assert_close(blank_probe, blank_probe_expected)
    t.testing.assert_close(my_probe, my_probe_expected)
    print("All tests in `test_my_probes` passed!")


def test_apply_scale(apply_scale: Callable):

    pos = 20
    resid = t.randn(1, 60, 512)
    flip_dir = t.randn(512)
    flip_dir_normed = flip_dir / flip_dir.norm()
    alpha = resid[0, pos] @ flip_dir_normed

    for scale in [0, 2, 8]:
        resid_expected = resid.clone()
        resid_expected[0, pos] -= (scale+1) * alpha * flip_dir_normed
        resid_actual = apply_scale(resid.clone(), flip_dir, scale, pos)
        t.testing.assert_close(resid_expected, resid_actual)

    print("All tests in `test_apply_scale` passed!")


def test_calculate_neuron_input_weights(
    calculate_neuron_input_weights: Callable,
    model: HookedTransformer, 
) -> Float[Tensor, "neurons rows cols"]:
    '''
    Returns tensor of the input weights for each neuron in the list, at each square on the board,
    projected along the corresponding probe directions.
    '''
    layer = 5
    neuron = 1393
    probe = t.randn(model.cfg.d_model, 8, 8, device=model.cfg.device)

    w_in = model.W_in[layer, :, neuron].detach()
    w_in_normed = w_in / w_in.norm(dim=0, keepdim=True)

    expected = einops.einsum(
        w_in_normed, probe,
        "d_model, d_model row col -> row col",
    )
    actual = calculate_neuron_input_weights(model, probe, layer, neuron = 1393)

    t.testing.assert_close(expected, actual)
    print("All tests in `test_calculate_neuron_input_weights` passed!")


def test_calculate_neuron_output_weights(
    calculate_neuron_output_weights: Callable,
    model: HookedTransformer, 
) -> Float[Tensor, "neurons rows cols"]:
    '''
    Returns tensor of the output weights for each neuron in the list, at each square on the board,
    projected along the corresponding probe directions.
    '''
    layer = 5
    neuron = 1393
    probe = t.randn(model.cfg.d_model, 8, 8, device=model.cfg.device)

    w_out = model.W_out[layer, neuron].detach()
    w_out_normed = w_out / w_out.norm(dim=0, keepdim=True)

    expected = einops.einsum(
        w_out_normed, probe,
        "d_model, d_model row col -> row col",
    )
    actual = calculate_neuron_output_weights(model, probe, layer, neuron = 1393)

    t.testing.assert_close(expected, actual)
    print("All tests in `test_calculate_neuron_output_weights` passed!")



def test_patching_metric(
    patching_metric: Callable,
    clean_log_probs: t.Tensor,
    corrupted_log_probs: t.Tensor
):
    device = t.device("cuda" if t.cuda.is_available() else "cpu")
    t.testing.assert_close(patching_metric(clean_log_probs), t.tensor(1.0).to(device))
    t.testing.assert_close(patching_metric(corrupted_log_probs), t.tensor(0.0).to(device))
    t.testing.assert_close(patching_metric(0.5 * (clean_log_probs + corrupted_log_probs)), t.tensor(0.5418457984924316).to(device))
    print("All tests in `test_patching_metric` passed!")