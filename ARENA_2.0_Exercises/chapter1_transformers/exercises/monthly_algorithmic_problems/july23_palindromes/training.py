from dataclasses import dataclass
from typing import Tuple, Optional
from tqdm import tqdm
import torch as t
from torch import Tensor
import torch.nn.functional as F
from torch.utils.data import DataLoader
import wandb
from itertools import chain
from monthly_algorithmic_problems.july23_palindromes.dataset import PalindromeDataset
from monthly_algorithmic_problems.july23_palindromes.model import create_model

@dataclass
class TrainArgs:
    half_length: int
    max_value: int
    trainset_size: int
    valset_size: int
    epochs: int
    batch_size: int
    lr: float
    weight_decay: float
    seed: int
    d_model: int
    d_head: int
    n_heads: int
    d_mlp: int
    normalization_type: Optional[str]
    use_wandb: bool
    device: str

class Trainer:
    def __init__(self, args: TrainArgs):
        self.args = args
        self.model = create_model(**args.__dict__)
        if args.use_wandb:
            wandb.init(project="palindromes")
            wandb.watch(self.model)

    def training_step(self, batch: Tuple[Tensor, Tensor]) -> t.Tensor:
        logits, is_palindrome = self._shared_train_validation_step(batch)
        logits = logits.to(t.float64).log_softmax(-1)
        loss = F.nll_loss(logits, is_palindrome)
        return loss
    
    def validation_step(self, batch: Tuple[Tensor, Tensor]) -> t.Tensor:
        logits, is_palindrome = self._shared_train_validation_step(batch)
        accuracy = (logits.argmax(-1) == is_palindrome).float().sum().item()
        return accuracy

    def _shared_train_validation_step(self, batch: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tensor]:
        toks, is_palindrome = batch
        toks = toks.to(self.args.device)
        is_palindrome = is_palindrome.to(self.args.device)
        logits = self.model(toks)[:, -1]
        return logits, is_palindrome

    def train_dataloader(self, seed: int):
        trainset = PalindromeDataset(size=self.args.trainset_size, max_value=self.args.max_value, half_length=self.args.half_length, seed=seed)
        return DataLoader(trainset, batch_size=self.args.batch_size, shuffle=True)
    
    def val_dataloader(self, seed: int):
        valset = PalindromeDataset(size=self.args.valset_size, max_value=self.args.max_value, half_length=self.args.half_length, seed=seed)
        return DataLoader(valset, batch_size=self.args.batch_size, shuffle=False)
    
    def configure_optimizers(self):
        optimizer = t.optim.Adam(self.model.parameters(), lr=self.args.lr, weight_decay=self.args.weight_decay)
        return optimizer



def train(args: TrainArgs):

    trainer = Trainer(args)
    optimizer = trainer.configure_optimizers()

    train_dataloader = trainer.train_dataloader(seed=args.seed)
    val_dataloader = trainer.val_dataloader(seed=args.seed+1)

    for epoch in range(args.epochs):

        progress_bar = tqdm(total=args.trainset_size//args.batch_size)

        # Training
        for batch in train_dataloader:
            # Optimization step on training set
            optimizer.zero_grad()
            loss = trainer.training_step(batch)
            loss.backward()
            optimizer.step()
            # Log variables, update progress bar
            if args.use_wandb: wandb.log({"training_loss": loss})
            progress_bar.update()
            progress_bar.set_description(f"Epoch {epoch:02}, Train loss = {loss:.4f}");
        
        # Validation
        with t.inference_mode():
            # Calculate accuracy on validation set
            accuracy_list = [trainer.validation_step(batch) for batch in val_dataloader]
            accuracy = sum(accuracy_list) / args.valset_size
            # Log variables, update progress bar
            if args.use_wandb: wandb.log({"test_accuracy": accuracy})
            progress_bar.set_description(f"Epoch {epoch:02}, Train loss = {loss:.4f}, Accuracy: {accuracy:.3f}");

    if args.use_wandb:
        wandb.finish()

    return trainer.model