{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 12066901.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 37180998.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1648877/1648877 [00:00<00:00, 12952781.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 6129513.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "test_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "train_dataset = datasets.MNIST('../data', train=False, transform=transform)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Unbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss is: -0.10 Test loss: -0.13, Test accuracy 13.55\n",
      "Train Loss is: -0.19 Test loss: -0.22, Test accuracy 22.19\n",
      "Train Loss is: -0.27 Test loss: -0.29, Test accuracy 29.70\n",
      "Train Loss is: -0.35 Test loss: -0.35, Test accuracy 35.83\n",
      "Train Loss is: -0.40 Test loss: -0.40, Test accuracy 40.49\n",
      "Train Loss is: -0.45 Test loss: -0.44, Test accuracy 44.53\n",
      "Train Loss is: -0.49 Test loss: -0.47, Test accuracy 47.70\n",
      "Train Loss is: -0.53 Test loss: -0.50, Test accuracy 50.58\n",
      "Train Loss is: -0.56 Test loss: -0.52, Test accuracy 52.80\n",
      "Train Loss is: -0.58 Test loss: -0.54, Test accuracy 54.80\n",
      "Train Loss is: -0.60 Test loss: -0.56, Test accuracy 56.56\n",
      "Train Loss is: -0.62 Test loss: -0.57, Test accuracy 58.13\n",
      "Train Loss is: -0.64 Test loss: -0.59, Test accuracy 59.44\n",
      "Train Loss is: -0.66 Test loss: -0.60, Test accuracy 60.64\n",
      "Train Loss is: -0.67 Test loss: -0.61, Test accuracy 61.59\n",
      "Train Loss is: -0.68 Test loss: -0.62, Test accuracy 62.54\n",
      "Train Loss is: -0.69 Test loss: -0.63, Test accuracy 63.44\n",
      "Train Loss is: -0.70 Test loss: -0.64, Test accuracy 64.22\n",
      "Train Loss is: -0.71 Test loss: -0.64, Test accuracy 64.90\n",
      "Train Loss is: -0.72 Test loss: -0.65, Test accuracy 65.61\n",
      "Train Loss is: -0.73 Test loss: -0.66, Test accuracy 66.19\n",
      "Train Loss is: -0.74 Test loss: -0.66, Test accuracy 66.77\n",
      "Train Loss is: -0.74 Test loss: -0.67, Test accuracy 67.31\n",
      "Train Loss is: -0.75 Test loss: -0.67, Test accuracy 67.80\n",
      "Train Loss is: -0.76 Test loss: -0.68, Test accuracy 68.23\n",
      "Train Loss is: -0.76 Test loss: -0.68, Test accuracy 68.62\n",
      "Train Loss is: -0.77 Test loss: -0.68, Test accuracy 69.03\n",
      "Train Loss is: -0.77 Test loss: -0.69, Test accuracy 69.36\n"
     ]
    }
   ],
   "source": [
    "# Creating a simple Multi layer perceptron\n",
    "# h = sigmoid(Wx + b)\n",
    "# y = softmax(Vh + c)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "Ni = 784\n",
    "Nh = 500\n",
    "No = 10\n",
    "\n",
    "W = np.random.normal(size = (Nh, Ni)) # Nh x Ni\n",
    "b = np.random.normal(size = (Nh,)) # Nh\n",
    "V = np.random.normal(size = (No, Nh)) # No x Nh\n",
    "c = np.random.normal(size = (No,))\n",
    "\n",
    "def sigmoid(x: np.ndarray):\n",
    "    return 1 / (1.0 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / (np.einsum(\"x->\", np.exp(x)) + 1e-8)\n",
    "\n",
    "x, target = 10, 20 # samples, no batch\n",
    "iterations = 100 # number of samples\n",
    "\n",
    "# parameters\n",
    "lr = 1e-4\n",
    "for i in range(iterations):\n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        x = data.numpy().reshape(Ni) # data shape [1,1,28,28] to [784,]\n",
    "        target = target.numpy() \n",
    "        target = np.eye(No)[target].reshape(No) # target shape [10,]\n",
    "        # Forward propagation\n",
    "\n",
    "        ha = np.einsum(\"hi,i->h\", W, x) + b #(Nh,)\n",
    "        h = sigmoid(ha) # (Nh,)\n",
    "        ya = np.einsum(\"oh,h->o\", V, h) + c #(No,)\n",
    "        y = softmax(ya) #(No,)\n",
    "\n",
    "        #print(f\"ha {ha.mean()}, h: {h.mean()}, ya: {ya.mean()}, y: {y.mean()}\")\n",
    "\n",
    "        # Loss\n",
    "        loss = -target * y\n",
    "        loss = np.einsum(\"k->\", loss)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backpropagation\n",
    "\n",
    "        dL_dya = y - target\n",
    "        dL_dV = np.einsum(\"o,h->oh\", dL_dya, h)\n",
    "        dL_dc = dL_dya\n",
    "        dL_dh = np.einsum(\"o,oh->h\", dL_dya, V) \n",
    "        dL_dha = dL_dh * np.einsum(\"h,h->h\", h, (1-h))\n",
    "        dL_dW = np.einsum(\"h,i->hi\", dL_dha, x)\n",
    "        dL_db = dL_dha\n",
    "\n",
    "        #print(f\"dya {dL_dya.mean()}, dV {dL_dV.mean()} dc: {dL_dc.mean()} dh {dL_dh.mean()} dha {dL_dha.mean()} dw {dL_dW.mean()}\")\n",
    "\n",
    "        # Update parameters\n",
    "        W = W - lr * dL_dW\n",
    "        b = b - lr * dL_db\n",
    "        V = V - lr * dL_dV\n",
    "        c = c - lr * dL_dc\n",
    "\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    for data, target in test_loader:\n",
    "        x = data.numpy().reshape(Ni) # data shape [1,1,28,28] to [784,]\n",
    "        target = target.numpy() \n",
    "        target = np.eye(No)[target].reshape(No) # target shape [10,]\n",
    "        # Forward propagation\n",
    "\n",
    "        ha = np.einsum(\"hi,i->h\", W, x) + b #(Nh,)\n",
    "        h = sigmoid(ha) # (Nh,)\n",
    "        ya = np.einsum(\"oh,h->o\", V, h) + c #(No,)\n",
    "        y = softmax(ya) #(No,)\n",
    "\n",
    "        # Loss\n",
    "        loss = -target * y\n",
    "        loss = np.einsum(\"k->\", loss)\n",
    "        test_loss += loss\n",
    "\n",
    "        # Accuracy\n",
    "        y_pred = np.argmax(y) #(1,)\n",
    "        y_true = np.argmax(target) #(1,)\n",
    "        test_accuracy += (y_pred == y_true).mean()\n",
    "\n",
    "    print(f\"Train Loss is: {total_loss/len(train_loader):.2f} Test loss: {test_loss/len(test_loader):.2f}, Test accuracy {test_accuracy/len(test_loader)*100:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previously, x is no batch, but now it is batched\n",
    "\n",
    "# Creating a simple Multi layer perceptron\n",
    "# h = sigmoid(Wx + b)\n",
    "# y = softmax(Vh + c)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "Ni = 784\n",
    "Nh = 500\n",
    "No = 10\n",
    "\n",
    "W = np.random.normal(size = (Nh, Ni)) # Nh x Ni\n",
    "b = np.random.normal(size = (Nh,)) # Nh\n",
    "V = np.random.normal(size = (No, Nh)) # No x Nh\n",
    "c = np.random.normal(size = (No,))\n",
    "\n",
    "def sigmoid(x: np.ndarray):\n",
    "    return 1 / (1.0 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / (np.einsum(\"bx->b\", np.exp(x)) + 1e-8)\n",
    "\n",
    "iterations = 100 # number of samples\n",
    "\n",
    "# parameters\n",
    "lr = 1e-4\n",
    "for i in range(iterations):\n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        x = data.numpy().reshape(batch_size, Ni) # data shape [bs,1,28,28] to [bs, 784]\n",
    "        target = target.numpy() \n",
    "        onehot = np.zeros((batch_size, No)) # target shape [bs, 10]\n",
    "        onehot[np.arange(batch_size), target] = 1\n",
    "        target = onehot\n",
    "        # Forward propagation\n",
    "\n",
    "        ha = np.einsum(\"hi,bi->bh\", W, x) + b #(bs, Nh)\n",
    "        h = sigmoid(ha) # (bs,Nh)\n",
    "        ya = np.einsum(\"oh,bh->bo\", V, h) + c #(bs,No)\n",
    "        y = softmax(ya) #(bs,No)\n",
    "\n",
    "        #print(f\"ha {ha.mean()}, h: {h.mean()}, ya: {ya.mean()}, y: {y.mean()}\")\n",
    "\n",
    "        # Loss\n",
    "        loss = -target * y\n",
    "        loss = np.einsum(\"bk->\", loss)\n",
    "        total_loss += loss/batch_size\n",
    "\n",
    "        # Backpropagation\n",
    "\n",
    "        dL_dya = y - target\n",
    "        dL_dV = np.einsum(\"bo,bh->oh\", dL_dya, h) / batch_size ## Need to divide as omitting leter in output will be summed\n",
    "        dL_dc = np.einsum(\"bo -> o\", dL_dya) / batch_size ### We divide batch because omitting letter in output will be sumed, then we need to divide\n",
    "        dL_dh = np.einsum(\"bo,oh->bh\", dL_dya, V) \n",
    "        dL_dha = dL_dh * np.einsum(\"bh,bh->bh\", h, (1-h))\n",
    "        dL_dW = np.einsum(\"bh,bi->hi\", dL_dha, x) / batch_size\n",
    "        dL_db = np.einsum(\"bh -> h\", dL_dha) / batch_size\n",
    "\n",
    "        #print(f\"dya {dL_dya.mean()}, dV {dL_dV.mean()} dc: {dL_dc.mean()} dh {dL_dh.mean()} dha {dL_dha.mean()} dw {dL_dW.mean()}\")\n",
    "\n",
    "        # Update parameters\n",
    "        W = W - lr * dL_dW\n",
    "        b = b - lr * dL_db\n",
    "        V = V - lr * dL_dV\n",
    "        c = c - lr * dL_dc\n",
    "\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    for data, target in test_loader:\n",
    "        x = data.numpy().reshape(batch_size, Ni) # data shape [1,1,28,28] to [784,]\n",
    "        target = target.numpy() \n",
    "        target = np.eye(No)[target].reshape(No) # target shape [10,]\n",
    "        # Forward propagation\n",
    "\n",
    "        ha = np.einsum(\"hi,i->h\", W, x) + b #(Nh,)\n",
    "        h = sigmoid(ha) # (Nh,)\n",
    "        ya = np.einsum(\"oh,h->o\", V, h) + c #(No,)\n",
    "        y = softmax(ya) #(No,)\n",
    "\n",
    "        # Loss\n",
    "        loss = -target * y\n",
    "        loss = np.einsum(\"k->\", loss)\n",
    "        test_loss += loss\n",
    "\n",
    "        # Accuracy\n",
    "        y_pred = np.argmax(y) #(1,)\n",
    "        y_true = np.argmax(target) #(1,)\n",
    "        test_accuracy += (y_pred == y_true).mean()\n",
    "\n",
    "    print(f\"Train Loss is: {total_loss/len(train_loader):.2f} Test loss: {test_loss/len(test_loader):.2f}, Test accuracy {test_accuracy/len(test_loader)*100:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
